{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "9a4ac318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from nltk.corpus import stopwords\n",
    "STOPS = set(stopwords.words('english'))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "STEMMER = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771d289",
   "metadata": {},
   "source": [
    "# Read in businesses from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "7b8fd802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 39s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#150,000 businesses\n",
    "fields_wanted = ['business_id','name','stars','review_count']\n",
    "review_threshold = 50 #amount of reviews a business needs in order to be considered\n",
    "read_line_limit = np.inf\n",
    "bus = pd.DataFrame(columns=fields_wanted)\n",
    "with open('../data/yelp_academic_dataset_business.json',encoding='utf-8') as d:\n",
    "    #print('num businesses: ' + str(len(d.readlines())))\n",
    "    counter = 0\n",
    "    for line in d:\n",
    "        L = json.loads(line)\n",
    "        if L['review_count'] < review_threshold:\n",
    "            continue\n",
    "        less_fields = {key: L[key] for key in fields_wanted }\n",
    "        bus.loc[counter] = less_fields\n",
    "        counter += 1\n",
    "        if counter == read_line_limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c50704",
   "metadata": {},
   "source": [
    "# Read in reviews from json file\n",
    "> - 6.9 million total reviews\n",
    "> -Review fields\n",
    "> - review_id (string), user_id (string), business_id (string), stars (float), useful (int - how many other users marked it as useful), funny (int), cool (int), text (string), date (ex. '2018-07-07 22:09:11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "0268aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23.2 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review_fields_wanted = ['review_id','business_id','text','stars']\n",
    "review_limit = 6900\n",
    "rev = pd.DataFrame(columns=review_fields_wanted)\n",
    "with open('../data/yelp_academic_dataset_review.json',encoding='utf-8') as d:\n",
    "    counter = 0\n",
    "    for line in d:\n",
    "        L = json.loads(line)\n",
    "        less_fields = {key: L[key] for key in review_fields_wanted }\n",
    "        rev.loc[counter] = less_fields\n",
    "        counter += 1\n",
    "        if counter == review_limit:\n",
    "            break\n",
    "            \n",
    "rev = rev.rename(columns = {'text':'_text','stars':'_stars'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d352e",
   "metadata": {},
   "source": [
    "# Pre-process review text data\n",
    "> - remove punctuation, tokenize words into list, \n",
    "> - remove stop words (ex. is, am, to, the, etc.) \n",
    "> - stem words using porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "76a85183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edit_limit = review_limit\n",
    "def preprocess(text_str):\n",
    "    #remove punctuation, make lowercase, and tokenize into a list of words\n",
    "    out_L = re.sub(r'[^\\w\\s]', '', text_str).lower().split()\n",
    "    #remove stop words and stem the words\n",
    "    out_L = [STEMMER.stem(x1) for x1 in out_L if x1 not in STOPS]\n",
    "    return out_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "c25874b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.8 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "original_text = rev['_text'].copy()\n",
    "Xtext = np.array(rev['_text'].map(preprocess))\n",
    "Ystars = np.array(rev['_stars'])\n",
    "#above is 46.4 MB of memory for 69,000 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b5a0b",
   "metadata": {},
   "source": [
    "# Naive bayesian classifier\n",
    "> - Positive review is >= 4 stars\n",
    "> - Negative review is <= 3 stars\n",
    "> - Uses a naive bayesian statistical model to classify positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "a7cf0aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 942 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#count occurrences of words in pos/neg reviews\n",
    "#word_L_ser is a series whose elements are lists of words\n",
    "#label_ser is a series of the same length as word_L_ser whose elements\n",
    "#are True if the corresponding word_L_ser entry is labeled positive, and False if negative\n",
    "#pos_freq is a dictionary whose keys are words and values are occurrences in positive reviews\n",
    "#neg_freq is the same as pos_freq but for negative reviews\n",
    "def pos_neg_count_L(word_L, label,pos_freq ,neg_freq ):\n",
    "    toadd = None\n",
    "    other = None\n",
    "    if label:\n",
    "        toadd = pos_freq\n",
    "        other = neg_freq\n",
    "    else:\n",
    "        toadd = neg_freq\n",
    "        other = pos_freq\n",
    "    for word in word_L:\n",
    "        if word in toadd:\n",
    "            toadd[word] += 1\n",
    "        else:\n",
    "            toadd[word] = 1\n",
    "        if word not in other:\n",
    "            other[word] = 0  \n",
    "                \n",
    "#Calculate probability of each word occurring in pos/neg review\n",
    "#Perform Laplacian (add 1) smoothing to the probabilities\n",
    "def count_to_prob(pos_freq,neg_freq):\n",
    "    pos_sum = sum(pos_freq.values())\n",
    "    pos_len = len(pos_freq)\n",
    "    pos_probs = {x:math.log((pos_freq[x]+1)/(pos_sum+pos_len)) for x in pos_freq.keys()}\n",
    "\n",
    "    neg_sum = sum(neg_freq.values())\n",
    "    neg_len = len(neg_freq)\n",
    "    neg_probs = {x : math.log((neg_freq[x]+1)/(neg_sum+neg_len)) for x in neg_freq.keys()}\n",
    "    \n",
    "    return pos_probs, neg_probs\n",
    "\n",
    "\n",
    "def calc_log_prior(Y):\n",
    "    pos_rev = np.sum(Y_train)\n",
    "    neg_rev = Y_train.shape[0] - pos_rev\n",
    "    return math.log(pos_rev/neg_rev)\n",
    "\n",
    "def predict_naive_bayes(X,pos_p_in,neg_p_in, log_prior):\n",
    "    pred = np.ones(X.shape[0])*log_prior\n",
    "    for i in range(X.shape[0]):\n",
    "        for word in X[i]:\n",
    "            pred[i] += pos_p_in.get(word,0) - neg_p_in.get(word,0)\n",
    "    return (pred >= 0).astype(int)\n",
    "\n",
    "def score_naive_bayes(H,Y):\n",
    "    acc = np.sum(H == Y)/Y.shape[0]\n",
    "    true_pos = np.sum(H == Y)\n",
    "    false_neg = np.sum(np.logical_and(H == 0, Y == 1))\n",
    "    false_pos = np.sum(np.logical_and(H == 1, Y == 0))\n",
    "    prec = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    return (acc,prec,recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b57e7",
   "metadata": {},
   "source": [
    "# Naive bayesian classifier results\n",
    "> - Train accuracy is 93%\n",
    "> - Test accuracy is 86%\n",
    "> - This was using the first 6900 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "b6bb419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [('accuracy', 0.928623188405797), ('precision', 0.9496109670248241), ('recall', 0.9767530487804879)]\n",
      "test:  [('accuracy', 0.8623188405797102), ('precision', 0.9008327024981075), ('recall', 0.9527622097678142)]\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_size = 0.2\n",
    "end_train = math.ceil((1-test_size)*Xtext.shape[0])\n",
    "X_train = Xtext[:end_train]\n",
    "Y_train = (Ystars[:end_train] >= 4).astype(int)\n",
    "X_test = Xtext[end_train:]\n",
    "Y_test = (Ystars[end_train:] >= 4).astype(int)\n",
    "\n",
    "pos_count2 = dict()\n",
    "neg_count2 = dict()\n",
    "add_count = np.vectorize(lambda x,y: pos_neg_count_L(x,y,pos_count2,neg_count2))\n",
    "add_count(X_train,Y_train)\n",
    "\n",
    "pos_rev_p, neg_rev_p = count_to_prob(pos_count,neg_count)\n",
    "\n",
    "H_train = predict_naive_bayes(X_train,pos_rev_p,neg_rev_p, calc_log_prior(Y_train))\n",
    "H_test = predict_naive_bayes(X_test,pos_rev_p,neg_rev_p, calc_log_prior(Y_train))\n",
    "\n",
    "\n",
    "print('train: ' , list(zip(['accuracy','precision','recall'],score_naive_bayes(H_train,Y_train))))\n",
    "print('test: ' , list(zip(['accuracy','precision','recall'],score_naive_bayes(H_test,Y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f7ce5b60897245265654a0241fbd2c54865ae31818f6b9e9653661495d2e924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
