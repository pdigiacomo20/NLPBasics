{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in train dataset from txt file\n",
    "> - file associates words and corresponding part of speech (POS) tags\n",
    "> - one such file can be acquired from Penn Treebank dataset (WSJ articles)\n",
    "> - only reads the first read_lim word/POS pairs (for small sample testing opt.)\n",
    "> - vocab_lim is a fraction between 0 and 1 which adds that fraction of the \n",
    ">   words to the vocabulary so that the model can account for the <unk> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "\n",
    "def read_pos_tags(path,read_lim,vocab_lim,verbose=False):\n",
    "    vocab = dict()\n",
    "    vocab[UNKNOWN_TOKEN] = 0\n",
    "    pos_all = dict()\n",
    "    tran = dict()\n",
    "    emi = dict()\n",
    "    last_pos = START_TOKEN\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if verbose:\n",
    "            print(len(lines))\n",
    "        for i, line in enumerate(lines):\n",
    "            line_L = line.split()\n",
    "            if not line_L:\n",
    "                line_L = [START_TOKEN,START_TOKEN]\n",
    "            \n",
    "            assert(len(line_L) == 2)\n",
    "\n",
    "            if i < int(len(lines)*vocab_lim):\n",
    "                if line_L[0] not in vocab:\n",
    "                    vocab[line_L[0]] = 1\n",
    "                else:\n",
    "                    vocab[line_L[0]] += 1\n",
    "            else:\n",
    "                if line_L[0] not in vocab:\n",
    "                    line_L[0] = UNKNOWN_TOKEN\n",
    "                vocab[line_L[0]] += 1\n",
    "\n",
    "            this_emi = (line_L[1],line_L[0])\n",
    "            if this_emi not in emi:\n",
    "                emi[this_emi] = 1\n",
    "            else:\n",
    "                emi[this_emi] += 1\n",
    "\n",
    "            this_tran = (last_pos,line_L[1])\n",
    "            if this_tran not in tran:\n",
    "                tran[this_tran] = 1\n",
    "            else:\n",
    "                tran[this_tran] += 1\n",
    "\n",
    "            if last_pos not in pos_all:\n",
    "                pos_all[last_pos] = 1\n",
    "            else:\n",
    "                pos_all[last_pos] += 1\n",
    "\n",
    "            if i == read_lim-1:\n",
    "                break\n",
    "\n",
    "            last_pos = line_L[1]\n",
    "        return vocab, pos_all, tran, emi\n",
    "\n",
    "\n",
    "#after counting word-POS associations, convert to log probabilities and\n",
    "#also perform smoothing to the probabilities\n",
    "def calc_probs(vocab, pos_all, tran, emi, k = 1):\n",
    "    num_vocab = len(vocab.keys())\n",
    "    num_pos = len(pos_all.keys())\n",
    "    for key, val in tran.items():\n",
    "        tran[key] = math.log((val + k)/(pos_all[key[0]] + k*num_pos))\n",
    "\n",
    "    for key, val in emi.items():\n",
    "        emi[key] = math.log((val + k)/(pos_all[key[0]] + k*num_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989860\n"
     ]
    }
   ],
   "source": [
    "p = '../data/WSJPartOfSpeech/POS_tags_train.txt'\n",
    "k = 1 #smoothing constant\n",
    "\n",
    "vocab, pos_all, tran, emi = read_pos_tags(p,np.inf,0.5, True)\n",
    "calc_probs(vocab, pos_all, tran, emi, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign a unique index to all POS tags for quick access\n",
    "#create dictionaries to convert between POS tag (string), its index, and back\n",
    "\n",
    "pos_to_ind = dict()\n",
    "for i,pos in enumerate(pos_all.keys()):\n",
    "    pos_to_ind[pos] = i - 1\n",
    "del pos_to_ind[START_TOKEN]\n",
    "\n",
    "ind_to_pos = {v:k for k,v in pos_to_ind.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm implementation\n",
    "> - sentence is a list of words (strings)\n",
    "> - tran: transition probabilities matrix\n",
    "> - emi: emissions probabilities matrix\n",
    "> - pos_to_ind and ind_to_pos_in are dictionaries that convert between POS and ind\n",
    "> - vocab_in is the vocabulary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viterbi_probs(sentence, tran, emi, pos_to_ind_in, ind_to_pos_in , vocab_in, k, verbose=False):\n",
    "    num_pos_v = len(pos_to_ind_in)\n",
    "    num_vocab_v = len(vocab)\n",
    "    vit_probs = np.empty(shape=(num_pos_v,len(sentence)))\n",
    "    back_ptrs = np.empty(shape=(num_pos_v,len(sentence)))\n",
    "\n",
    "    #initialize first column of viterbi probabilities matrix\n",
    "    for key,val in pos_to_ind_in.items():\n",
    "        vit_probs[val,0] = 0\n",
    "\n",
    "        #add transition log prob from start token\n",
    "        if (START_TOKEN,key) in tran:\n",
    "            vit_probs[val,0] += tran[(START_TOKEN,key)] \n",
    "        else: \n",
    "            vit_probs[val,0] += math.log((k)/(pos_all[START_TOKEN] + k*num_pos_v))\n",
    "        \n",
    "        #add emission log prob given the first word\n",
    "        if (key,sentence[0]) in emi:\n",
    "            vit_probs[val,0] += emi[(key,sentence[0])]\n",
    "        else:\n",
    "            vit_probs[val,0] += math.log((k)/(pos_all[key] + k*num_vocab_v))\n",
    "\n",
    "    #viterbi forward pass\n",
    "    for w_ind_last,word in enumerate(sentence[1:]):\n",
    "        if word not in vocab_in:\n",
    "            word = UNKNOWN_TOKEN\n",
    "        for pos_this,ind_this in pos_to_ind_in.items():\n",
    "            best_prob = np.NINF\n",
    "            best_ind = -1\n",
    "            for pos_last,ind_last in pos_to_ind_in.items():\n",
    "                prob = vit_probs[ind_last,w_ind_last]\n",
    "                if (pos_last,pos_this) in tran:\n",
    "                    prob += tran[(pos_last,pos_this)] \n",
    "                else:\n",
    "                    prob += math.log((k)/(pos_all[pos_last] + k*num_pos_v))\n",
    "                \n",
    "                if (pos_this,sentence[w_ind_last+1]) in emi:\n",
    "                    prob += emi[(pos_this,sentence[w_ind_last+1])]\n",
    "                else:\n",
    "                    prob += math.log((k)/(pos_all[pos_this] + k*num_vocab_v))\n",
    "\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_ind = ind_last\n",
    "            vit_probs[ind_this,w_ind_last+1] = best_prob\n",
    "            back_ptrs[ind_this,w_ind_last+1] = best_ind\n",
    "\n",
    "    tag_inds = list()\n",
    "    tag_inds.append(np.argmax(vit_probs[:,len(sentence)-1]))\n",
    "    for w_ind in range(len(sentence)-1,0,-1):\n",
    "        next_ind = back_ptrs[int(tag_inds[-1]),w_ind]\n",
    "        tag_inds.append(next_ind)\n",
    "    if(verbose):\n",
    "        print(vit_probs)\n",
    "        print(back_ptrs)\n",
    "    tags_output = [ind_to_pos_in[x] for x in reversed(tag_inds)]\n",
    "\n",
    "    return tags_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Viterbi prediction\n",
    "> - Give pre-defined transition and emission matrices to ensure viterbi function\n",
    ">   works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6  1.35 2.75]\n",
      " [0.35 1.65 2.  ]\n",
      " [0.7  1.55 2.5 ]]\n",
      "[[0.00000000e+000 1.00000000e+000 1.00000000e+000]\n",
      " [0.00000000e+000 2.00000000e+000 1.00000000e+000]\n",
      " [1.24610723e-306 0.00000000e+000 0.00000000e+000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'V', 'N']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = ['happy','finds','optimists']\n",
    "tran1 = {\n",
    "            ('N','N'):0.1,\n",
    "            ('N','V'):0.25,\n",
    "            ('N','A'):0.65,\n",
    "            ('V','N'):0.75,\n",
    "            ('V','V'):0.15,\n",
    "            ('V','A'):0.1,\n",
    "            ('A','N'):0.35,\n",
    "            ('A','V'):0.2,\n",
    "            ('A','A'):0.45,\n",
    "            (START_TOKEN,'N'):0.2,\n",
    "            (START_TOKEN,'V'):0.3,\n",
    "            (START_TOKEN,'A'):0.5,\n",
    "        }\n",
    "\n",
    "emi1 = {\n",
    "            ('N','happy'):0.4,\n",
    "            ('N','finds'):0.25,\n",
    "            ('N','optimists'):0.35,\n",
    "            ('V','happy'):0.05,\n",
    "            ('V','finds'):0.75,\n",
    "            ('V','optimists'):0.2,\n",
    "            ('A','happy'):0.2,\n",
    "            ('A','finds'):0.3,\n",
    "            ('A','optimists'):0.5,\n",
    "        }\n",
    "\n",
    "pos_ind1 = {'N':0,'V':1,'A':2}\n",
    "ind_pos1 = {0:'N',1:'V',2:'A'}\n",
    "vocab1 = {'happy','finds','optimists'}\n",
    "get_viterbi_probs(sentence1, tran1, emi1, pos_ind1, ind_pos1, vocab1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads the test set from a separate txt file\n",
    "#vocab_in is the vocab dicitonary from the test set\n",
    "#read_lim limits the number of word/POS pairs used in the test set\n",
    "def read_test_set(path, vocab_in, read_lim = np.inf,start_token=START_TOKEN):\n",
    "    last_pos = start_token\n",
    "    words_test = []\n",
    "    tags_test = []\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == read_lim:\n",
    "                break\n",
    "            line_L = line.split()\n",
    "            if not line_L:\n",
    "                line_L = [start_token,start_token]\n",
    "            try:\n",
    "                assert(len(line_L) == 2)\n",
    "            except:\n",
    "                print(line_L)\n",
    "\n",
    "            if line_L[0] not in vocab_in:\n",
    "                line_L[0] = UNKNOWN_TOKEN\n",
    "\n",
    "            words_test.append(line_L[0])\n",
    "            tags_test.append(line_L[1])\n",
    "\n",
    "        \n",
    "        return words_test, tags_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test = read_test_set('../data/WSJPartOfSpeech/POS_tags_test.txt',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaks up list of words (in word_L) into sentences and applies viterbi \n",
    "#algorithm on each sentence.\n",
    "def vit_predict(word_L,get_vit_in, tran_in, emi_in, pos_to_ind_in, ind_to_pos_in,vocab_in,k=1):\n",
    "    out_tags = list()\n",
    "    \n",
    "    end_sent_inds = [-1]\n",
    "    end_sent_inds += [i for i,word in enumerate(word_L) if word == START_TOKEN]\n",
    "    end_sent_inds.append(len(word_L))\n",
    "\n",
    "    for i in range(len(end_sent_inds)-2):\n",
    "        start_ind = end_sent_inds[i]\n",
    "        end_ind = end_sent_inds[i+1]\n",
    "        sent = word_L[start_ind+1:end_ind]\n",
    "        out_tags += get_vit_in(sent, tran_in, emi_in, pos_to_ind_in, ind_to_pos_in,vocab_in,k)\n",
    "        out_tags.append(START_TOKEN)\n",
    "\n",
    "    return out_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_test = vit_predict(X_test,get_viterbi_probs,tran,emi,pos_to_ind,ind_to_pos,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278300307430067"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This viterbi algorithm has 92% accuracy on the test set\n",
    "Y_test_no_starts = [el for el in Y_test if el != START_TOKEN]\n",
    "h_test_no_starts = [el for el in h_test if el != START_TOKEN]\n",
    "\n",
    "Y_test_np = np.array(Y_test_no_starts)\n",
    "h_test_np = np.array(h_test_no_starts)\n",
    "\n",
    "np.sum((Y_test_np == h_test_np).astype(int))/len(Y_test_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env_YelpPredictor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54933d5d0454cb54ea8cf4e7b3c099269c704adbee2a5b35f5a5ea4d0f5219ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
